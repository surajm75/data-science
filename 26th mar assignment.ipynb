{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7cfee6c",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7e0dbc",
   "metadata": {},
   "source": [
    "simple linear regression has one independent and one dependent feature while simple linear regression has multi independent and one dependent feature.\n",
    "For example, a simple linear regression model could be used to model the relationship between a person's weight (dependent variable) and their height (independent variable).\n",
    "and a multiple linear regression model could be used to model the relationship between a house's price (dependent variable) and its size, number of bedrooms, and location (independent variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea4ece",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f1161",
   "metadata": {},
   "source": [
    "The assumptions of linear regression are:\n",
    "\n",
    "Linearity: The relationship between the dependent variable and independent variables is linear. This means that the change in the dependent variable is proportional to the change in the independent variable.\n",
    "\n",
    "Independence: The observations are independent of each other. This means that the value of one observation should not be affected by the value of another observation.\n",
    "\n",
    "Homoscedasticity: The variance of the residuals (the difference between the predicted value and the actual value) is constant across all levels of the independent variables. This means that the spread of the residuals should be similar for all values of the independent variable.\n",
    "\n",
    "Normality: The residuals are normally distributed. This means that the distribution of the residuals should be symmetric and follow a normal distribution.\n",
    "\n",
    "No multicollinearity: There is no perfect linear relationship between the independent variables. This means that the independent variables should not be highly correlated with each other.\n",
    "\n",
    "\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, we can use various diagnostic plots and statistical tests, such as:\n",
    "\n",
    "Scatter plots: To check linearity, we can plot the dependent variable against each independent variable and look for a linear relationship.\n",
    "\n",
    "Residual plots: To check homoscedasticity, we can plot the residuals against the predicted values or independent variables and look for a constant spread of the residuals.\n",
    "\n",
    "Normal probability plots: To check normality, we can plot the residuals against a normal distribution and look for a straight line.\n",
    "\n",
    "Cook's distance: To check for influential observations, we can use Cook's distance, which measures the influence of each observation on the regression coefficients.\n",
    "\n",
    "Variance inflation factor (VIF): To check for multicollinearity, we can use the VIF, which measures the degree of correlation between the independent variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e789b",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857cd4d9",
   "metadata": {},
   "source": [
    "The slope coefficient represents the change in the dependent variable for each unit increase in the independent variable. For example, if the slope coefficient is 0.5, it means that for each unit increase in the independent variable, the dependent variable increases by 0.5 units. The slope can be positive or negative, indicating a positive or negative relationship between the variables, respectively.\n",
    "\n",
    "The intercept coefficient represents the value of the dependent variable when the independent variable(s) is equal to zero. This means that the intercept represents the value of the dependent variable when there is no effect of the independent variable(s). The intercept can be interpreted as the starting point or baseline for the dependent variable.\n",
    "\n",
    "Here's an example of how to interpret the slope and intercept in a real-world scenario:\n",
    "\n",
    "Suppose we want to model the relationship between a person's age (independent variable) and their annual income (dependent variable). We collect data on 100 individuals and fit a linear regression model to the data. The model we obtain is:\n",
    "\n",
    "Annual Income = 40,000 + 1,000 * Age\n",
    "\n",
    "In this model, the intercept coefficient is 40,000, which means that the predicted annual income for someone who is 0 years old (i.e., a newborn) is $40,000. This can be interpreted as the baseline income for someone who has not yet started working.\n",
    "\n",
    "The slope coefficient is 1,000, which means that for each year increase in age, the predicted annual income increases by $1,000. This indicates a positive relationship between age and income, meaning that as people get older, they tend to earn more.\n",
    "\n",
    "Using this model, we can predict the annual income of a person based on their age. For example, if someone is 35 years old, we can plug in 35 for age in the equation and get:\n",
    "\n",
    "Annual Income = 40,000 + 1,000 * 35 = $75,000\n",
    "\n",
    "This means that we would predict this person's annual income to be $75,000 based on their age."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562ea861",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0532b805",
   "metadata": {},
   "source": [
    "\n",
    "Gradient descent is a numerical optimization algorithm used to find the minimum of a function. In machine learning, it is commonly used to find the optimal values of the parameters in a model that minimize the error between the predicted and actual values.\n",
    "\n",
    "The basic idea of gradient descent is to iteratively adjust the parameters of the model in the direction of the negative gradient of the loss function. The loss function measures the difference between the predicted and actual values, and the negative gradient indicates the direction of steepest descent.\n",
    "\n",
    "The gradient descent algorithm starts with an initial set of parameter values and updates the parameters in each iteration until the minimum of the loss function is reached. The update rule for the parameters is:\n",
    "\n",
    "θi = θi - α * ∂J(θ)/∂θi\n",
    "\n",
    "where θi is the i-th parameter, α is the learning rate (a hyperparameter that controls the step size of the update), J(θ) is the loss function, and ∂J(θ)/∂θi is the partial derivative of the loss function with respect to the i-th parameter.\n",
    "\n",
    "The algorithm iteratively updates the parameters until the convergence criteria are met, such as a fixed number of iterations or a small change in the loss function.\n",
    "\n",
    "Gradient descent is used in machine learning to train models such as linear regression, logistic regression, neural networks, and others. By minimizing the loss function, gradient descent allows the model to learn from the data and make accurate predictions on new data.\n",
    "\n",
    "There are several variants of gradient descent, such as stochastic gradient descent (SGD) and mini-batch gradient descent, that are designed to improve the efficiency and speed of the algorithm. SGD uses a random subset of the data to estimate the gradient, while mini-batch gradient descent uses a small batch of data to estimate the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ebe6bf",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af95e319",
   "metadata": {},
   "source": [
    "Multiple linear regression is a statistical model that examines the linear relationship between a dependent variable and two or more independent variables. It is an extension of simple linear regression, which examines the linear relationship between a dependent variable and a single independent variable.\n",
    "\n",
    "In multiple linear regression, the model is expressed as:\n",
    "\n",
    "Y = β0 + β1X1 + β2X2 + … + βpXp + ε\n",
    "\n",
    "where Y is the dependent variable, X1, X2, …, Xp are the independent variables, β0 is the intercept, β1, β2, …, βp are the coefficients of the independent variables, and ε is the error term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa3b49e",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f774cdf7",
   "metadata": {},
   "source": [
    "Multicollinearity refers to a situation where there is a high degree of correlation among the predictor variables in a multiple linear regression model. This can be a problem because it can lead to unstable and unreliable estimates of the regression coefficients, and make it difficult to determine which variables are actually contributing to the prediction.\n",
    "\n",
    "There are several ways to detect multicollinearity. One common method is to calculate the correlation matrix among all the predictor variables, and check for high values of correlation coefficients between pairs of variables. Another approach is to calculate the variance inflation factor (VIF) for each predictor variable, which measures the extent to which the variance of the estimated coefficient for that variable is inflated due to multicollinearity. A VIF value greater than 5 or 10 is often considered to be indicative of a significant level of multicollinearity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44a47f0",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11bb60f",
   "metadata": {},
   "source": [
    "polynomial regression model is basically non-linear relationship between independent and dependent variable.\n",
    "Polynomial regression is a type of regression analysis in which the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an nth degree polynomial. It is a non-linear form of regression analysis that can capture more complex relationships between the variables.\n",
    "\n",
    "The polynomial regression model can be written as:\n",
    "\n",
    "Y = b0 + b1X + b2X^2 + ... + bn*X^n\n",
    "\n",
    "where Y is the dependent variable, X is the independent variable, b0 is the intercept, b1, b2, ..., bn are the coefficients for the nth degree terms, and n is the degree of the polynomial.\n",
    "\n",
    "The main difference between polynomial and linear regression is the degree of the equation used to model the relationship between the variables. While linear regression models assume a linear relationship between the variables, polynomial regression models can capture more complex, nonlinear relationships. Polynomial regression can be useful when the relationship between the variables is not linear and cannot be modeled by a simple straight line. However, it can also be more prone to overfitting if the degree of the polynomial is too high, which can lead to poor generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f466d1",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2caeb7a",
   "metadata": {},
   "source": [
    "Advantages of polynomial regression compared to linear regression:\n",
    "\n",
    "It can capture nonlinear relationships between the independent and dependent variables.\n",
    "It provides a better fit to the data when the relationship between the variables is curvilinear or has a U-shaped pattern.\n",
    "It can provide a more accurate prediction of the dependent variable for new values of the independent variable.\n",
    "\n",
    "Disadvantages of polynomial regression compared to linear regression:\n",
    "\n",
    "It can overfit the data if the degree of the polynomial is too high, which can lead to poor generalization to new data.\n",
    "It can be more difficult to interpret than linear regression because the coefficients for each degree term can have different signs and magnitudes.\n",
    "\n",
    "Situations where polynomial regression may be preferred over linear regression include:\n",
    "\n",
    "When the relationship between the independent and dependent variables is not linear but has a curved pattern.\n",
    "When there is prior knowledge or a theory that suggests a polynomial relationship between the variables.\n",
    "When the goal is to achieve a better fit to the data and improve the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aec2fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db394b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
