{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e34edd45",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b062a14",
   "metadata": {},
   "source": [
    "KNN stands for K-Nearest Neighbors. It is a supervised machine learning algorithm that can be used for both classification and regression tasks.\n",
    "\n",
    "It works by finding the k most similar data points to a new data point and then predicting the label of the new data point based on the labels of the k nearest neighbors.\n",
    "\n",
    "The value of k is a hyperparameter that needs to be set by the user. A larger value of k will typically result in a more accurate prediction, but it will also be more computationally expensive. A smaller value of k will be less computationally expensive, but it may not be as accurate.\n",
    "\n",
    "The KNN algorithm is a non-parametric algorithm. This means that it does not make any assumptions about the underlying distribution of the data. As a result, the KNN algorithm is often able to make accurate predictions even when the data is not linearly separable.\n",
    "\n",
    "The KNN algorithm is a simple and efficient algorithm that can be used for a variety of tasks. However, it can be computationally expensive for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d510df7",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e288e5",
   "metadata": {},
   "source": [
    "\n",
    "The value of k in KNN is a hyperparameter that needs to be set by the user. There is no one-size-fits-all answer to this question, as the best value of k will depend on the specific dataset and the desired accuracy.\n",
    "\n",
    "Here are some common approaches to choosing the value of k:\n",
    "\n",
    "Cross-validation: This is a common approach to choosing hyperparameters. In cross-validation, the dataset is split into a training set and a test set. The KNN algorithm is trained on the training set and then evaluated on the test set. The value of k that results in the best accuracy on the test set is chosen.\n",
    "\n",
    "Grid search: This is a more exhaustive approach to choosing hyperparameters. In grid search, a range of values for k is considered. The KNN algorithm is trained and evaluated for each value of k. The value of k that results in the best accuracy is chosen.\n",
    "\n",
    "Expert knowledge: In some cases, it may be possible to use expert knowledge to choose the value of k. For example, if the data is known to be linearly separable, then a smaller value of k may be sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9f3a80",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499dd220",
   "metadata": {},
   "source": [
    "The main difference between KNN classifier and KNN regressor is the task they are used for.\n",
    "\n",
    "KNN classifier is used for classification tasks, where the goal is to predict the category or class of a new data point. For example, a KNN classifier could be used to predict whether a tumor is benign or malignant.\n",
    "\n",
    "KNN regressor is used for regression tasks, where the goal is to predict a continuous value for a new data point. For example, a KNN regressor could be used to predict the price of a house or the number of sales made"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f2382a",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f68fea8",
   "metadata": {},
   "source": [
    "\n",
    "There are many ways to measure the performance of KNN. Some of the most common metrics include:\n",
    "\n",
    "Accuracy: This is the most common metric for measuring the performance of a classifier. It is calculated as the number of correctly classified data points divided by the total number of data points.\n",
    "\n",
    "Precision: This metric measures the fraction of predicted positive data points that are actually positive.\n",
    "\n",
    "Recall: This metric measures the fraction of actual positive data points that are predicted as positive.\n",
    "\n",
    "F1 score: This metric is a weighted average of precision and recall.\n",
    "\n",
    "Mean squared error (MSE): This metric is used to measure the performance of a regressor. It is calculated as the average squared difference between the predicted values and the actual values.\n",
    "\n",
    "Root mean squared error (RMSE): This metric is the square root of the MSE. It is a more intuitive measure of error than the MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f743b2f",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b835a0",
   "metadata": {},
   "source": [
    "The curse of dimensionality is a phenomenon that occurs when the number of features in a dataset increases. As the number of features increases, the distance between data points becomes more spread out, and it becomes more difficult to find the k nearest neighbors. This can lead to a decrease in the accuracy of KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f1bb83",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d8b16",
   "metadata": {},
   "source": [
    "There are several ways to handle missing values in KNN:\n",
    "\n",
    "Ignore the data points with missing values. This is the simplest approach, but it can lead to a decrease in the accuracy of KNN.\n",
    "\n",
    "Impute the missing values. This can be done using a variety of methods, such as mean imputation, median imputation, or multiple imputation.\n",
    "\n",
    "Use a distance metric that is robust to missing values. Some distance metrics, such as the Manhattan distance and the Minkowski distance, are less sensitive to missing values than the Euclidean distance.\n",
    "\n",
    "Use a modified KNN algorithm that can handle missing values. There are several modified KNN algorithms that have been proposed, such as the KNN with replacement algorithm and the KNN with multiple nearest neighbors algorithm.\n",
    "\n",
    "The best approach to handling missing values in KNN will depend on the specific dataset and the desired accuracy. It is important to experiment with different approaches to find the one that works best for the specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c397bdc",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f650889",
   "metadata": {},
   "source": [
    "The KNN classifier and regressor are both non-parametric machine learning algorithms that can be used for a variety of tasks. However, they have different strengths and weaknesses, and they are better suited for different types of problems.\n",
    "\n",
    "The KNN classifier is better for classification tasks, where the goal is to predict the category or class of a new data point. This is because the KNN classifier uses the labels of the k nearest neighbors to make a prediction. The KNN regressor is better for regression tasks, where the goal is to predict a continuous value for a new data point. This is because the KNN regressor uses the values of the k nearest neighbors to make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d384c77",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915c95b",
   "metadata": {},
   "source": [
    "The KNN algorithm is a simple and efficient machine learning algorithm that can be used for both classification and regression tasks. However, it also has some limitations.\n",
    "\n",
    "Strengths of KNN algorithm:\n",
    "\n",
    "Simple and easy to understand: The KNN algorithm is relatively simple to understand and implement. It does not require any complex mathematical or statistical knowledge.\n",
    "\n",
    "Non-parametric: The KNN algorithm is a non-parametric algorithm, which means that it does not make any assumptions about the underlying distribution of the data. This makes it a versatile algorithm that can be used for a variety of problems.\n",
    "\n",
    "Robust to outliers: The KNN algorithm is relatively robust to outliers. This means that it is not significantly affected by the presence of a few data points that are very different from the rest of the data.\n",
    "\n",
    "Efficient: The KNN algorithm is a relatively efficient algorithm, especially for small datasets. It does not require any complex calculations or optimization procedures.\n",
    "\n",
    "Weaknesses of KNN algorithm:\n",
    "\n",
    "Sensitive to the choice of k: The KNN algorithm is sensitive to the choice of the hyperparameter k. This means that the accuracy of the algorithm can vary significantly depending on the value of k.\n",
    "\n",
    "Not suitable for high-dimensional data: The KNN algorithm can be computationally expensive for high-dimensional data. This is because the algorithm needs to calculate the distance between the new data point and all of the training data points.\n",
    "\n",
    "Not suitable for noisy data: The KNN algorithm can be sensitive to noise in the data. This is because the algorithm uses the distance between data points to make predictions, and noise can distort the distances.\n",
    "\n",
    "How to address the weaknesses of KNN algorithm:\n",
    "\n",
    "The weaknesses of the KNN algorithm can be addressed by using some techniques:\n",
    "\n",
    "Choosing the right value of k: The value of k can be chosen using cross-validation. Cross-validation is a technique that is used to evaluate the performance of an algorithm on a held-out set of data.\n",
    "\n",
    "Using a distance metric that is robust to noise: There are several distance metrics that are more robust to noise than the Euclidean distance. Some examples of these metrics include the Manhattan distance and the Minkowski distance.\n",
    "\n",
    "Using dimensionality reduction: Dimensionality reduction is a technique that can be used to reduce the number of features in a dataset. This can make the KNN algorithm more efficient and less sensitive to noise.\n",
    "\n",
    "Using ensemble methods: Ensemble methods combine the predictions of multiple algorithms to improve the overall accuracy. The KNN algorithm can be combined with other algorithms, such as decision trees, to improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69498d2",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e12cda",
   "metadata": {},
   "source": [
    "The Euclidean distance and the Manhattan distance are two of the most commonly used distance metrics in KNN.\n",
    "\n",
    "The Euclidean distance is the straight-line distance between two points. It is calculated as the square root of the sum of the squared differences between the corresponding features of the two points.\n",
    "\n",
    "The Manhattan distance is the sum of the absolute differences between the corresponding features of the two points.\n",
    "\n",
    "The main difference between the Euclidean distance and the Manhattan distance is that the Euclidean distance takes into account the square of the differences between the corresponding features, while the Manhattan distance takes into account the absolute differences between the corresponding features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66fd412",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469ce8ee",
   "metadata": {},
   "source": [
    "Feature scaling is the process of normalizing the features of a dataset so that they have a similar range of values. This is important for KNN because the distance between two data points is calculated based on the values of their features. If the features are not scaled, then the distance between two data points can be misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3097ea6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c8792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35161723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e47334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
