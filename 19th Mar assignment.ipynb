{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947f77ac",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f90ec7b",
   "metadata": {},
   "source": [
    "MinMax Scaler shrinks the data within the given range, usually of 0 to 1. It transforms data by scaling features to a given range. It scales the values to a specific value range without changing the shape of the original distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7b9a74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        ],\n",
       "       [0.27272727, 0.625     ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.75      ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [[11, 2], [3, 7], [0, 10], [11, 8]]\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31229151",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04418183",
   "metadata": {},
   "source": [
    "Unit Vector Scaling scales the features so that each data point has a length of 1. This technique is useful when the angle between the data points is essential. However, MinMax Scaler shrinks the data within the given range, usually of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9302529b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98386991, 0.17888544],\n",
       "       [0.3939193 , 0.91914503],\n",
       "       [0.        , 1.        ],\n",
       "       [0.80873608, 0.5881717 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "data = [[11, 2], [3, 7], [0, 10], [11, 8]]\n",
    "scaled_data = normalize(data)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063f5dc",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d94af9",
   "metadata": {},
   "source": [
    "Principal Component Analysis is an unsupervised learning algorithm that is used for the dimensionality reduction in machine learning. It is a statistical process that converts the observations of correlated features into a set of linearly uncorrelated features with the help of orthogonal transformation.\n",
    "PCA is used to visualize multidimensional data. It is used to reduce the number of dimensions in healthcare data. PCA can help resize an image.It is also used for finding patterns in data of high dimension in the field of finance, data mining, bioinformatics, psychology, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c654562",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f0398",
   "metadata": {},
   "source": [
    "PCA reduces the dimensionality without losing information from any features. Speed up the learning algorithm (with lower dimension). Address the multicollinearity issue (all principal components are orthogonal to each other). Help visualize data with high dimensionality (after reducing the dimension to 2 or 3).\n",
    "PCA in machine learning is used to visualize multidimensional data. In healthcare data to explore the factors that are assumed to be very important in increasing the risk of any chronic disease. PCA helps to resize an image. PCA is used to analyze stock data and forecasting data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4db42",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f03389",
   "metadata": {},
   "source": [
    "This scaling method can be applied to features such as price, rating, and delivery time in the dataset for building a recommendation system for a food delivery service. Here's how Min-Max scaling can be applied:\n",
    "Identify the numerical features: First, identify the numerical features in the dataset that need to be scaled. In this case, it could be features such as price, rating, and delivery time.\n",
    "1.Compute the minimum and maximum values: Calculate the minimum and maximum values for each of the numerical features. These values will be used to scale the features to the desired range (typically between 0 and 1).\n",
    "2.Apply the Min-Max scaling formula: For each numerical feature, apply the Min-Max scaling formula, which is given by:\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "where X is the original value of the feature, X_min is the minimum value of the feature, and X_max is the maximum value of the feature.\n",
    "3.Scale the features: Substitute the original values of the numerical features into the Min-Max scaling formula to obtain the scaled values. This will ensure that all the numerical features are scaled to the range between 0 and 1.\n",
    "4.Update the dataset: Replace the original values of the numerical features in the dataset with their corresponding scaled values. This will result in a preprocessed dataset where the numerical features are scaled using Min-Max scaling.\n",
    "5.Use the preprocessed dataset: The preprocessed dataset with Min-Max scaled features can now be used as input for building the recommendation system. The scaled features will have a consistent range, which can help in avoiding any bias due to differences in the original scales of the features. The recommendation system can then use this preprocessed dataset to make accurate and reliable recommendations for food delivery based on the scaled features such as price, rating, and delivery time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a988094b",
   "metadata": {},
   "source": [
    "#code\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c433a534",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb50f01",
   "metadata": {},
   "source": [
    "Here's how PCA can be used to reduce the dimensionality of the dataset for building a model to predict stock prices:\n",
    "1.Identify the features: First, identify the features in the dataset that need to be reduced using PCA. These could be company financial data and market trends, such as revenue, earnings, market capitalization, interest rates, economic indicators, etc.\n",
    "2.Standardize the features: Since PCA is sensitive to the scale of the features, it's important to standardize the features to have zero mean and unit variance. This can be done by subtracting the mean from each feature and dividing by the standard deviation.\n",
    "3.Compute the covariance matrix: Calculate the covariance matrix for the standardized features. The covariance matrix provides information about the relationships between pairs of features, and it will be used in PCA to determine the principal components.\n",
    "4.Compute the eigenvectors and eigenvalues: Compute the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions in the feature space along which the data varies the most, and the eigenvalues represent the amount of variance explained by each eigenvector. Sort the eigenvectors in descending order of their corresponding eigenvalues.\n",
    "5.Select the top k eigenvectors: Choose the top k eigenvectors that correspond to the largest eigenvalues. These k eigenvectors will be the principal components that capture most of the variance in the original dataset. The value of k is determined by the desired level of dimensionality reduction.\n",
    "6.Project the data onto the principal components: Project the standardized features onto the selected k principal components to obtain a reduced-dimensional representation of the data. This can be done by multiplying the standardized features with the selected eigenvectors.\n",
    "7.Reconstruct the data: If needed, the reduced-dimensional data can be reconstructed back to the original feature space using the selected eigenvectors. This can be done by multiplying the projected data with the transpose of the selected eigenvectors and adding back the mean of the original features.\n",
    "8.Use the reduced-dimensional data: The reduced-dimensional data obtained from PCA can now be used as input for building the model to predict stock prices. The reduced feature space can help in reducing the complexity of the model, improving computational efficiency, and potentially avoiding overfitting. The model can then be trained and evaluated using the reduced-dimensional data, and the predictions can be made accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c87de",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ea977cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformed data in range of -1 to 1 is [-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]\n"
     ]
    }
   ],
   "source": [
    "data =  [1, 5, 10, 15, 20]\n",
    "import numpy as np\n",
    "maximum = np.max(data)\n",
    "minimum = np.min(data)\n",
    "scaled_data = []\n",
    "for i in data:\n",
    "    scaled = (i-minimum)/(maximum - minimum)\n",
    "    scaled_data.append((scaled*2) -1)\n",
    "print(f\"The transformed data in range of -1 to 1 is {scaled_data}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fcefd5",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ac2a65",
   "metadata": {},
   "source": [
    "Here's an overview of the steps to perform feature extraction using PCA for the given dataset with features [height, weight, age, gender, blood pressure]:\n",
    "1.Data Preparation: Preprocess the dataset by handling missing values, normalizing/standardizing the features if needed, and encoding categorical variables like gender.\n",
    "2.Covariance Matrix: Calculate the covariance matrix for the features in the dataset. The covariance matrix provides information about the relationships and variances among the features.\n",
    "3.Eigenvectors and Eigenvalues: Compute the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions or principal components along which the data varies the most, and the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "4.Explained Variance Ratio: Calculate the explained variance ratio, which represents the proportion of total variance in the dataset that is explained by each principal component. This can be computed by dividing each eigenvalue by the sum of all eigenvalues.\n",
    "5.Cumulative Explained Variance Ratio: Calculate the cumulative explained variance ratio, which represents the cumulative proportion of total variance explained by a certain number of principal components. This can be obtained by summing up the explained variance ratios for the principal components in a cumulative manner.\n",
    "6.Retaining Principal Components: Analyze the cumulative explained variance ratio and choose the number of principal components to retain based on the desired level of dimensionality reduction. A common threshold is to retain principal components that explain a significant portion of the total variance, such as 80% or 90%. This can help retain the most important information in the data while reducing the dimensionality.\n",
    "7.Interpretation: Interpret the retained principal components in terms of the original features and their respective loadings (eigenvectors). These loadings represent the weights or contributions of the original features to each principal component and can provide insights into the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2be608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
